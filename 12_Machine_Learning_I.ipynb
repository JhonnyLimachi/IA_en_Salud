{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwBR3n25J3O/pJQ778llta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JhonnyLimachi/IA_en_Salud/blob/main/12_Machine_Learning_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<hr style=\"border: none; height: 1px; background-color: #007bff; margin: 20px 0;\">\n",
        "  <h3 style=\"color: #343a40; margin: 0; font-size: 20px;\"><strong>DS-201: MACHINE LEARNING I</strong></h2>\n",
        "  <p style=\"color: #6c757d; margin: 5px 0 0; font-size: 14px;\"><strong>Instrutor:</strong> Jhonny Limachi-Choque, MSc.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "9QsubsA77-KY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-Nearest Neighbors (k-NN)\n",
        "\n",
        "O k-NN é reconhecido como um dos algoritmos de classificação mais intuitivos e simples em machine learning. Diferente de outros métodos que \"aprendem\" padrões em um conjunto de dados, o k-NN opera sob a premissa de que dados similares tendem a se agrupar no espaço de características. Isso significa que o k-NN utiliza a distância entre vetores de *features* para realizar suas predições, dependendo diretamente dessa métrica para classificar novos pontos.\n",
        "\n",
        "Considere pares $(X_1, Y_1), (X_2, Y_2), \\dots, (X_n, Y_n)$ em $\\mathbb{R}^d \\times \\{1, 2\\}$, onde $X$ representa os atributos dos pontos de dados em um espaço d-dimensional, e $Y$ é o rótulo da classe de $X$, indicando a qual das duas classes o ponto pertence.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/carlosfab/blog-sigmoidal/3ade0bb023e48c5acf9ebe0bccf454a72886de6f/Implementing%20k-Nearest%20Neighbors%20for%20RR%20Lyrae%20Star%20Classification/knn.png\" width=800></center>\n",
        "\n",
        "Cada $X$ condicional a $Y=r$ segue uma distribuição de probabilidade $P_r$ para $r=1, 2$. Isso significa que, dado um rótulo de classe específico, a distribuição dos pontos de dados em $X$ segue um padrão específico, descrito pela distribuição $P_r$.\n",
        "\n",
        "Dada uma norma $\\|\\cdot\\|$ em $\\mathbb{R}^d$ e um ponto $x \\in \\mathbb{R}^d$, ordenamos os dados de treinamento tal que $(X_{(1)}, Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})$ de forma que $\\|X_{(1)} - x\\| \\leq \\dots \\leq \\|X_{(n)} - x\\|$. Em outras palavras, reorganizamos os dados de treinamento com base na proximidade de cada ponto $X_i$ ao ponto de consulta $x$, do mais próximo ao mais distante.\n",
        "\n",
        "## Taxonomia do k-NN\n",
        "\n",
        "O algoritmo k-NN pertence à família dos métodos de **aprendizagem supervisionada**, onde as previsões são realizadas com base em um conjunto de exemplos rotulados. Mais especificamente, o k-NN se enquadra na categoria de **aprendizado baseado em instâncias** (ou \"lazy learning\"). Ao contrário de algoritmos como Regressão Linear ou Redes Neurais, que constroem modelos explícitos durante a fase de treinamento, o k-NN mantém os dados de treinamento em memória e só realiza cálculos no momento da predição.\n",
        "\n",
        "É importante diferenciar o k-NN de outros algoritmos populares que possuem nomes semelhantes, como o K-Means. Enquanto o **k-NN** é um algoritmo de **classificação ou regressão supervisionada**, o **K-Means** é um algoritmo de **agrupamento (clustering) não supervisionado**. Além disso, o k-NN é considerado um **lazy learner**, pois a generalização só ocorre no momento da predição, enquanto o K-Means é um **eager learner**, já que constrói um modelo durante a fase de treinamento.\n",
        "\n",
        "### Por que usar o k-NN hoje?\n",
        "\n",
        "Apesar da existência de algoritmos mais sofisticados, como SVM, Redes Neurais e Gradient Boosting, o k-NN continua sendo relevante em diversos cenários. A sua simplicidade o torna ideal para situações em que:\n",
        "\n",
        "- Os dados têm uma distribuição não linear ou são altamente complexos, dificultando o ajuste de um modelo paramétrico.\n",
        "- Queremos um algoritmo que ofereça uma solução intuitiva e fácil de explicar.\n",
        "- O tempo de treinamento não é uma prioridade, mas a simplicidade e a transparência são desejáveis.\n",
        "\n",
        "Além disso, o k-NN serve como um excelente **baseline** para comparação com outros métodos, dado seu desempenho razoável em muitos problemas e sua capacidade de capturar padrões locais, o que o torna flexível para certas tarefas.\n",
        "\n",
        "Portanto, mesmo com avanços em algoritmos mais modernos, o k-NN se mantém relevante devido à sua aplicabilidade em cenários onde a simplicidade e a flexibilidade são importantes."
      ],
      "metadata": {
        "id": "qI2xXF5s8YRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intuição por trás do k-NN\n",
        "\n",
        "Imagine que você queira classificar os ingredientes da sua despensa com base em duas *features* que você supõe, possam ser mensuradas pelo seu paladar perspicaz: doçura e crocância. Cada ingrediente foi cuidadosamente provado e mensurado em uma escala arbitrária, e o resultado pode ser observado na imagem abaixo, retirada do livro [Machine Learning with R (Brett Lantz, 2019)](https://amzn.to/3NLvN6x).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/carlosfab/blog-sigmoidal/3ade0bb023e48c5acf9ebe0bccf454a72886de6f/Implementing%20k-Nearest%20Neighbors%20for%20RR%20Lyrae%20Star%20Classification/ingredients_knn.png\" width=800>\n",
        "\n",
        "As frutas, geralmente mais doces, agrupam-se na parte mais afastada da origem em relação ao eixo x, enquanto as verduras, menos doces e mais crocantes, e as proteínas, menos doces e menos crocantes, se agrupam em áreas distintas do gráfico. Este padrão visual nos fornece uma pista clara: a doçura e a crocância são bons indicadores para classificar um ingrediente disponíveis na nossa lista.\n",
        "\n",
        "Agora, digamos que temos uma fruta desconhecida e queremos classificá-la usando k-NN. Começamos localizando a fruta no gráfico baseando-nos em sua doçura e crocância. Em seguida, selecionamos um número 'k' de pontos de dados mais próximos - neste caso, os ingredientes mais próximos no gráfico.\n",
        "\n",
        "Se escolhermos, por exemplo, k=3, identificaremos os três ingredientes mais próximos da nossa fruta desconhecida no gráfico. Se dois deles forem 'frutas' e um for 'vegetal', então, pela regra da maioria, o k-NN classificará a fruta desconhecida como 'fruta'. Esse processo é intuitivo e espelha a forma como muitas vezes fazemos escolhas baseadas em semelhanças óbvias.\n",
        "\n",
        "Obviamente esse foi um exemplo didático e intuitivo. Mas para lidar com problemas reais, é essencial escolher um valor adequado para 'k', além de uma métrica de distância que reflita a natureza e dimensionalidade dos dados.\n",
        "\n",
        "## Definição Matemática\n",
        "\n",
        "Dado um conjunto de treinamento composto por pares $(X_1, Y_1), (X_2, Y_2), \\dots, (X_n, Y_n)$ em um espaço de $d$ dimensões, onde $X_i$ representa um vetor de características e $Y_i$ o rótulo da classe correspondente, o objetivo do k-NN é prever o rótulo $Y$ para um novo ponto de teste $X$.\n",
        "\n",
        "Para isso, o algoritmo calcula as distâncias entre $X$ e todos os pontos de treinamento $X_i$, utilizando uma métrica de distância, como a distância Euclidiana:\n",
        "\n",
        "$$d(X, X_i) = \\sqrt{\\sum_{j=1}^{d} (X_j - X_{ij})^2}$$\n",
        "\n",
        "Uma vez que as distâncias são calculadas, o algoritmo seleciona os $k$ vizinhos mais próximos, isto é, os $k$ pontos $X_i$ com as menores distâncias em relação a $X$. A predição é feita com base na classe mais frequente (moda) entre esses $k$ vizinhos:\n",
        "\n",
        "$$\\hat{Y} = \\text{argmax}_{r \\in \\{1, 2, \\dots, K\\}} \\sum_{i=1}^{k} \\mathbb{1}(Y_i = r)$$\n",
        "\n",
        "Onde $\\mathbb{1}(Y_i = r)$ é uma função indicadora que vale 1 se $Y_i = r$ e 0 caso contrário."
      ],
      "metadata": {
        "id": "g7hifWnR_BCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Como escolher o k\n",
        "\n",
        "A escolha de 'k' no algoritmo k-NN pode variar significativamente dependendo do conjunto de dados. Não existe uma regra única. Mas com base na experiência, aqui estão algumas diretrizes gerais:\n",
        "\n",
        "- Um $k$ pequeno, como 3 ou 5, geralmente é uma boa escolha para evitar a influência de *outliers* e manter a decisão localizada próxima ao ponto de interrogação. No entanto, um valor muito baixo pode ser sensível a ruídos nos dados.\n",
        "\n",
        "- Um $k$ maior oferece uma decisão mais \"democrática\", considerando mais vizinhos, o que pode ser útil para conjuntos de dados com muita variação. Contudo, um valor muito grande pode suavizar demais as fronteiras de decisão, levando a classificações menos precisas.\n",
        "\n",
        "Uma técnica comum é usar validação cruzada para experimentar diferentes valores de $k$ e escolher aquele que oferece o melhor desempenho no conjunto de validação. Isso ajuda a encontrar um equilíbrio entre subajuste e sobreajuste. Acima de tudo, a escolha deve levar em conta os *insights* gerados durante a fase do *business understanding*."
      ],
      "metadata": {
        "id": "sliMpXHZBL4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jJI8sjr79ED"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo fictício de classificação com k-NN\n",
        "\n",
        "Na propulsão de foguetes, a escolha do combustível é um dos fatores mais cruciais que afeta o desempenho e a eficiência da missão. Entre as várias características que influenciam essa escolha, a **densidade energética** é particularmente importante, pois define a quantidade de energia disponível por unidade de massa do combustível.\n",
        "\n",
        "A densidade energética é medida em **MJ/kg** (megajoules por quilograma) e fornece uma estimativa direta da eficiência potencial do combustível para gerar empuxo. Veja alguns exemplos e valores aproximados:\n",
        "\n",
        "* **Querosene de aviação**: densidade energética de aproximadamente **43 MJ/kg**.\n",
        "* **Hidrogênio líquido**: densidade energética de aproximadamente **120 MJ/kg**.\n",
        "* **Metano líquido**: densidade energética de aproximadamente **55 MJ/kg**.\n",
        "\n",
        "\n",
        "Neste notebook, vamos usar o algoritmo k-NN (K-Nearest Neighbors) para classificar diferentes tipos de foguetes com base em dois atributos importantes:\n",
        "\n",
        "1. **Densidade Energética** do combustível (em MJ/kg).\n",
        "2. **Distância** que o foguete pode alcançar (em km).\n",
        "\n",
        "Essas duas características influenciam o desempenho do foguete, e o k-NN nos ajudará a prever o tipo de foguete com base nos dados de treino fornecidos.\n",
        "\n",
        "### Passo 1: Importar as bibliotecas necessárias\n",
        "Primeiro, vamos importar as bibliotecas que usaremos ao longo do notebook."
      ],
      "metadata": {
        "id": "2FJZrQjhDtSn"
      }
    }
  ]
}