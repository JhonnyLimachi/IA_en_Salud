{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2jqK0KcecOScoDHqRW5g2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JhonnyLimachi/IA_en_Salud/blob/main/11_Machine_Learning_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<hr style=\"border: none; height: 1px; background-color: #007bff; margin: 20px 0;\">\n",
        "  <h3 style=\"color: #343a40; margin: 0; font-size: 20px;\"><strong>DS-201: MACHINE LEARNING I</strong></h2>\n",
        "  <p style=\"color: #6c757d; margin: 5px 0 0; font-size: 14px;\"><strong>Instrutor:</strong> Jhonny Limachi-Choque, MSc.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "9QsubsA77-KY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-Nearest Neighbors (k-NN)\n",
        "\n",
        "O k-NN é reconhecido como um dos algoritmos de classificação mais intuitivos e simples em machine learning. Diferente de outros métodos que \"aprendem\" padrões em um conjunto de dados, o k-NN opera sob a premissa de que dados similares tendem a se agrupar no espaço de características. Isso significa que o k-NN utiliza a distância entre vetores de *features* para realizar suas predições, dependendo diretamente dessa métrica para classificar novos pontos.\n",
        "\n",
        "Considere pares $(X_1, Y_1), (X_2, Y_2), \\dots, (X_n, Y_n)$ em $\\mathbb{R}^d \\times \\{1, 2\\}$, onde $X$ representa os atributos dos pontos de dados em um espaço d-dimensional, e $Y$ é o rótulo da classe de $X$, indicando a qual das duas classes o ponto pertence.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/carlosfab/blog-sigmoidal/3ade0bb023e48c5acf9ebe0bccf454a72886de6f/Implementing%20k-Nearest%20Neighbors%20for%20RR%20Lyrae%20Star%20Classification/knn.png\" width=800></center>\n",
        "\n",
        "Cada $X$ condicional a $Y=r$ segue uma distribuição de probabilidade $P_r$ para $r=1, 2$. Isso significa que, dado um rótulo de classe específico, a distribuição dos pontos de dados em $X$ segue um padrão específico, descrito pela distribuição $P_r$.\n",
        "\n",
        "Dada uma norma $\\|\\cdot\\|$ em $\\mathbb{R}^d$ e um ponto $x \\in \\mathbb{R}^d$, ordenamos os dados de treinamento tal que $(X_{(1)}, Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})$ de forma que $\\|X_{(1)} - x\\| \\leq \\dots \\leq \\|X_{(n)} - x\\|$. Em outras palavras, reorganizamos os dados de treinamento com base na proximidade de cada ponto $X_i$ ao ponto de consulta $x$, do mais próximo ao mais distante.\n",
        "\n",
        "## Taxonomia do k-NN\n",
        "\n",
        "O algoritmo k-NN pertence à família dos métodos de **aprendizagem supervisionada**, onde as previsões são realizadas com base em um conjunto de exemplos rotulados. Mais especificamente, o k-NN se enquadra na categoria de **aprendizado baseado em instâncias** (ou \"lazy learning\"). Ao contrário de algoritmos como Regressão Linear ou Redes Neurais, que constroem modelos explícitos durante a fase de treinamento, o k-NN mantém os dados de treinamento em memória e só realiza cálculos no momento da predição.\n",
        "\n",
        "É importante diferenciar o k-NN de outros algoritmos populares que possuem nomes semelhantes, como o K-Means. Enquanto o **k-NN** é um algoritmo de **classificação ou regressão supervisionada**, o **K-Means** é um algoritmo de **agrupamento (clustering) não supervisionado**. Além disso, o k-NN é considerado um **lazy learner**, pois a generalização só ocorre no momento da predição, enquanto o K-Means é um **eager learner**, já que constrói um modelo durante a fase de treinamento.\n",
        "\n",
        "### Por que usar o k-NN hoje?\n",
        "\n",
        "Apesar da existência de algoritmos mais sofisticados, como SVM, Redes Neurais e Gradient Boosting, o k-NN continua sendo relevante em diversos cenários. A sua simplicidade o torna ideal para situações em que:\n",
        "\n",
        "- Os dados têm uma distribuição não linear ou são altamente complexos, dificultando o ajuste de um modelo paramétrico.\n",
        "- Queremos um algoritmo que ofereça uma solução intuitiva e fácil de explicar.\n",
        "- O tempo de treinamento não é uma prioridade, mas a simplicidade e a transparência são desejáveis.\n",
        "\n",
        "Além disso, o k-NN serve como um excelente **baseline** para comparação com outros métodos, dado seu desempenho razoável em muitos problemas e sua capacidade de capturar padrões locais, o que o torna flexível para certas tarefas.\n",
        "\n",
        "Portanto, mesmo com avanços em algoritmos mais modernos, o k-NN se mantém relevante devido à sua aplicabilidade em cenários onde a simplicidade e a flexibilidade são importantes."
      ],
      "metadata": {
        "id": "qI2xXF5s8YRX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jJI8sjr79ED"
      },
      "outputs": [],
      "source": []
    }
  ]
}